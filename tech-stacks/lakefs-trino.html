<!DOCTYPE html>
<html lang="en">
<head>
    <title>Data Mesh Architecture: Databricks</title>
    <meta charset="utf-8">
    <meta name="description" content="How to build a data mesh architecture with Databricks" />
    <meta name="keywords" content="data mesh, data mesh architecture, domain-driven data analytics, data analytics, domain-driven design, domain ownership, data as a product, data product, federated governance, self-serve data platform, data platform">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="twitter:card" content="summary_large_image" />
    <meta name="twitter:site" content="@innoq" />
    <meta name="twitter:title" content="Data Mesh Architecture: Databricks" />
    <meta name="twitter:description" content="How to build a data mesh architecture with Databricks" />
    <meta name="twitter:image" content="https://www.datamesh-architecture.com/images/databricks_card.png" />
    <meta name="twitter:image:alt" content="Data Mesh Architecture: Domains are in the center and teams do analytics on their own. They build and interconnect with data products. A data platform team and a enablement team help. Global policies are agreed through federated governance." />
    <meta property="og:url" content="https://datamesh-architecture.com" />
    <meta property="og:title" content="Data Mesh Architecture: Databricks" />
    <meta property="og:description" content="How to build a data mesh architecture with Databricks" />
    <meta property="og:image" content="https://www.datamesh-architecture.com/images/databricks_card.png" />

    <link rel="preload" as="font" type="font/woff2" href="https://www.innoq.com/assets/MarkPro-Book.woff2?cachebuster=2" crossorigin="">
    <link rel="preload" as="font" type="font/woff2" href="https://www.innoq.com/assets/MarkPro-Bold.woff2?cachebuster=2" crossorigin="">
    <link rel="preload" as="font" type="font/woff2" href="https://www.innoq.com/assets/MarkPro-Heavy.woff2?cachebuster=2" crossorigin="">
    <link rel="stylesheet" href="../css/style.css" />
    <link rel="stylesheet" href="../css/0.9.3_css_bulma.css" />
    <link rel="stylesheet" href="../css/font-awesome_6.0.0_css_all.css"/>
    <link rel="shortcut icon" type="image/x-icon" href="/favicon.ico">
</head>
<body>
<nav class="navbar is-dark" role="navigation" aria-label="dropdown navigation">
    <div class="container">
        <div class="navbar-brand">
                    <span class="navbar-burger" data-target="navbarMenuHeroA">
            <span></span>
            <span></span>
            <span></span>
          </span>
        </div>
        <div id="navbarMenuHeroA" class="navbar-menu">

            <div class="navbar-end">


                <a href="/#why" class="navbar-item">
                    Why
                </a>
                <a href="/#what-is-data-mesh" class="navbar-item">
                    What
                </a>
                <div class="navbar-item has-dropdown is-hoverable">
                    <a href="/#how-to-design-a-data-mesh" class="navbar-link is-arrowless">
                        How
                    </a>
                    <div class="navbar-dropdown" id="navbarMenuArchitectureDropdown">
                        <a href="/#how-to-design-a-data-mesh" class="navbar-item">Data Mesh Architecture</a>
                        <hr class="navbar-divider">
                        <a href="/#data-product" class="navbar-item">Data Product</a>
                        <a href="/#federated-governance" class="navbar-item">Federated Governance</a>
                        <a href="/#analytical-data" class="navbar-item">Analytical Data</a>
                        <a href="/#ingesting" class="navbar-item">Ingesting</a>
                        <a href="/#clean-data" class="navbar-item">Clean Data</a>
                        <a href="/#analytics" class="navbar-item">Analytics</a>
                        <a href="/#data-platform" class="navbar-item">Data Platform</a>
                        <a href="/#enabling-team" class="navbar-item">Enabling Team</a>
                    </div>
                </div>
                <a href="/#mesh" class="navbar-item">
                    Mesh
                </a>
                <div class="navbar-item has-dropdown is-hoverable">
                    <a href="/#tech-stacks" class="navbar-link is-arrowless">
                        Tech Stacks
                    </a>
                    <div class="navbar-dropdown" id="navbarMenuTechStackDropdown">
                        <a href="/tech-stacks/google-cloud-bigquery.html" class="navbar-item">
                            Google Cloud BigQuery
                        </a>
                        <a href="/tech-stacks/aws-s3-athena.html" class="navbar-item">
                            AWS S3 and AWS Athena
                        </a>
                        <a href="/tech-stacks/azure-synapse-analytics.html" class="navbar-item">
                            Azure Synapse Analytics
                        </a>
                        <a href="/tech-stacks/dbt-snowflake.html" class="navbar-item">
                            dbt and Snowflake
                        </a>
                        <a href="/tech-stacks/databricks.html" class="navbar-item">
                            Databricks
                        </a>
                    </div>
                </div>
                <div class="navbar-item has-dropdown is-hoverable">
                    <a href="/#domain-teams-journey" class="navbar-link is-arrowless">
                        Start the Journey
                    </a>
                    <div class="navbar-dropdown is-right" id="navbarMenuTransformationDropdown">
                        <a href="/#domain-teams-journey" class="navbar-item">
                            Domain Team’s Journey
                        </a>
                        <a href="/#data-teams-journey" class="navbar-item">
                            Data Team’s Journey
                        </a>
                        <a href="/real-world-learnings.html" class="navbar-item">
                            Real World Learnings
                        </a>
                    </div>
                </div>
            </div>
        </div>
    </div>
</nav>

<div class="container">

    <section class="section">

        <nav class="breadcrumb" aria-label="breadcrumbs">
            <ul>
                <li><a href="/">Data Mesh Architecture</a></li>
                <li><a href="/#tech-stacks">Tech Stacks</a></li>
                <li class="is-active"><a href="#" aria-current="page">Databricks</a></li>
            </ul>
        </nav>

        <h1 class="title">LakeFS and Trino</h1>

        <div class="notification is-info is-light">
            Data mesh is primarily an organizational approach, and that's why you can't buy a data mesh from a vendor. Technology, however, is important still as it acts as an enabler for data mesh, and only useful and easy to use solutions will lead to domain teams' acceptance. The available offerings of cloud providers already provide a sufficient set of good self-serve data services to let you form a data platform for your data mesh. We want to show which services can be used to get started.
        </div>

        <div class="content">
            <p>
                LakeFS with Trino is an alternative open source tech stack for the data mesh approach.
                </br></br>
                <a href="https://lakefs.io">LakeFS</a> is a git-like data versioning tool that recently has emerged high popularity.
                Thereby lakeFS is used like a layer on top of the common (s3) storage solution.
                With lakeFS, <strong>common git operations</strong> like branching and are added the storage solution.
                This branching functionality provides this stack with <strong>isolation without copying data</strong> and can be used to create lasting snapshots of the data or in the case of a data mesh, a data product.
            </p>
            <p>
                <a href="https://trino.io">Trino</a> is a fast and distributed query engine.
                Thereby trino works with a hive metastore to support SQL-based data processing.
                With trino its also easy to switch between databases due to the support of using multiple databases in a single SQL-statement.
            </p>
            <p>
                <a href="https://trino.io">MinIO</a> is the complementary object storage.
                MinIO is a good choice because of the flexible s3-compatibility, which allows using most of the large cloud providers like AWS, Azure and Google.
            </p>

            <strong>WIP:</strong>
            <img src="../images/lakefs_trino.webp" alt="Data Mesh Architecture with LakeFS and Trino" style="width: 100%">

            <!--
            <ul>
                 <li>LakeFS with Trino is an alternative open source tech stack</li>
                <li>The stack consists of three main frameworks:</li>

                <ul>
                    <li>MinIO and an s3-compatible object storage</li>
                    <li>LakeFS as a layer on top of the object storage</li>
                    <li>Trino is the distibuted query engine that can combine multiple datawarehouses</li>
                </ul>

                <li>Fully open source and highly customizable through expansions like</li>
                <ul>
                    <li>dbt which gives various benefits as described in the snowflake stack</li>
                    <li>openlineage which is especially useful if the data lineage is necessary</li>
                </ul>

                <li>can be initialized in a local setup or use cloud resources (Object Storage, Compute, Access) by a cloud provider</li>
            </ul>
            -->

            <!-- intro setup with jnbs and dbt -->
            <div class="columns">
                <div class="column">
                    <p>
                        This stack is about the combination of LakeFS with trino, which has the main benefit of providing
                        data versioning to this data mesh stack. Furthermore, this stack comes with a very high customizability,
                        which is both beneficial as well as a challenging factor through a quite high mental load.
                    </p>
                    <p>
                        Overall this stack is wrapped into a docker environment that can be configured in the docker compose file.
                        With the containers running the transformation and cleaning of data can be done with trino.
                    </p>
                </div>
                <div class="column">
                    <figure class="image">
                        <a href="../images/lakeFS_dockercontainers.png" class="glightbox">
                            <img src="../images/lakeFS_dockercontainers.png" alt="Docker container overview">
                        </a>
                    </figure>
                </div>
            </div>


            <!-- intro setup with jnbs and dbt -->
            <div class="columns">
                <div class="column">
                    <p>
                        The trino engine can be used through jupyter notebooks or any database tool of choice.
                        </br>
                        We recommend using the database tools only for exploratory analysis, but jupyter notebooks can be used for exploratory
                        as well as production purposes. If dbt is also used, it can be used for persistent production workflows.
                    </p>
                    <p>
                        After the data product design, we suggest a data product workflow that starts with an exploratory phase in database tool or jupyter notebooks.
                        The statements that lead to an accepted data product can always be easily transfered into a persistant jupyter notebook or dbt.
                    </p>
                </div>
                <div class="column">
                    <figure class="image">
                        <a href="../images/lakeFS_jupyternotebook.png" class="glightbox">
                            <img src="../images/lakeFS_jupyternotebook.png" alt="Jupyter notebook overview">
                        </a>
                    </figure>
                </div>
            </div>

            <!-- data product = trino Tabelle die mit Daten versorgt wird -->
            <div class="columns">
                <div class="column">
                    <p>
                        With data mesh, every domain team manages their own jupyter notebooks and resulting tables in their database.
                        When publishing a data product, usually a table or view, it should be tagged as <em>data_product</em>. Thereby its beneficial
                        to also put them into a dedicated schema like <code>data_products</code>, which can be safed in a separate folder in lakeFS.
                        <br/>
                        To publish the data product that or any sub folder with the raw data could be published via lakeFS, but the recommended way is to use trino.
                        When using data products from other domains, with trino, they can be easily referred through the ability to combine tables
                        from multiple sources in the same sql statement.
                    </p>
                    <p>
                        Also, tests can be used to verify the expectations on an imported data set.
                        Furthermore with lakeFS you can use pre- and post-merge or commit hooks to automate simple scripts for policies that you agreed on in the
                        federated governance group.
                    </p>
                </div>
                <div class="column">
                    <figure class="image">
                        <a href="../images/unknown.png" class="glightbox">
                            <img src="../images/unknown.png" alt="dbeaver/lakefs tree">
                        </a>
                    </figure>
                </div>
            </div>

            <!-- complementary
            <div class="columns">
                <div class="column">
                    <p>
                        <strong>Creation of data products:</strong>
                    <ul>
                        <li> in notebooks: highly customizable through code, including import, transform, validation, and export;</li>
                            <ul>
                                <li>lots of freedom (sql, python, ...);</li>
                                <li>custom step to publish data product description may be helpful (dynamic infos)</li>
                            </ul>
                        <li> with dbt: sql statements in dbt models </li>
                        <ul>
                            <li> sql only but benefits of dbt included</li>
                            <li> custom and predefined tests</li>
                            <li> dbt models as metadata information</li>
                        </ul>
                    </ul>
                    </p>
                    <p>
                        <strong>The sharing of data products:</strong>
                        <ul>
                            <li> easy: done within trino via just accessing database tables of another teams database</li>
                            <li> intermediate: with access control in lakeFS and sharing of snapshot branches whereby metadata needs to be published accordingly</li>
                        </ul>
                    </p>
                    <p style="margin-top:1cm;">
                        <strong>Downside:</strong>
                    <li> Mental load is high-medium to very high through the infinite amount of customizability</li>
                    </p>
                </div>

                <div class="column">
                    <figure class="image">
                        <a href="../images/lakeFS_trino_query.png" class="glightbox">
                            <img src="../images/lakeFS_trino_query.png" alt="notebook trino statement">
                        </a>
                    </figure>
                    <figure class="image">
                        <a href="../images/lakeFS_dbt_query.png" class="glightbox">
                            <img src="../images/lakeFS_dbt_query.png" alt="dbt model statement">
                        </a>
                    </figure>
                </div>
            </div>
            -->

            <!-- stores = lakefs in branches, darunter MinIO was auf s3 liegt -->
            <div class="columns">
                <div class="column">
                    <p>
                        In our experience we used a local MinIO setup that uses the local storage, but
                        this can be replaced by <strong>any s3-compatible object storage</strong> for scaling.
                        This allows a simple initial setup as well as beeing a viable production solution.
                    </p>
                    <p>
                        The object storage gets covered by <strong>lakeFS, a version control system for data</strong>.
                        With lakeFS the underlying object storage is saving various non-assignable data files that are mapped by
                        lakeFS to construct a filesystem for the user (see lakeFS picture).
                        Each team typically should work in its own lakeFS repository.
                    </p>
                </div>
                <div class="column">
                    <figure class="image">
                        <a href="../images/lakeFS_minio_homescreen.png" class="glightbox">
                            <img src="../images/lakeFS_minio_homescreen.png" alt="MinIO Demoscreen">
                        </a>
                    </figure>
                    <figure class="image">
                        <a href="../images/lakeFS_lakefs_homescreen.png" class="glightbox">
                            <img src="../images/lakeFS_lakefs_homescreen.png" alt="LakeFS Demoscreen">
                        </a>
                    </figure>
                </div>
            </div>

            <!-- data versioning =  infos über lakefs -->
            <div class="columns">
                <div class="column">
                    <p>
                        The <strong>speciality</strong> of this stack is the <strong>data versioning</strong> that LakeFS provides.
                    </p>
                    <p>
                        LakeFS is a data version control system which provides git-like operations for your data lake.
                        Normally, through the branching it is easy to isolate specific snapshots of the current data in the repository.
                        With that an exact snapshot of the data can be produced by simply creating a branch.
                        For a data mesh, this branch could be either used as a data product, which gets shared with the current data,
                        or as a exploration or test branch, where the developer can test out things, with the mindset, that he can't corrupt
                        any data in the main or production branch.
                    </br>
                        A major benefit of <strong>branching</strong> is that <strong>no data gets copied</strong> in the process.
                    </p>
                </div>
                <div class="column">
                    <figure class="image">
                        <a href="../images/tbd.png" class="glightbox">
                            <img src="../images/tbd.png" alt="Lakefs branches picture">
                        </a>
                    </figure>
                </div>
            </div>


            <!-- transformationen = local or cloud compute instance -->
            <div class="columns">
                <div class="column">
                    <p>
                        The storage stack then gets combined with <strong>trino (form. presto) a fast and distributed query engine</strong>.
                        This is also the framework that uses the local or cloud computing resources, which again supports a simple initial
                        setup while also beeing a viable production solution.
                    </p>
                    <p>
                        A benefit of trino is that it allows to combine tables from multiple databases in one sql statement.
                        This gets useful in the data mesh context. One the one hand, this is very useful during the setup process,
                        when the database gets built or transfered, because, if the old database is compatible with trino, it can be
                        directly transfered or even fully kept if needed.
                        </br>
                        On the other hand it is useful when publishing data products, because it gives the domain teams a little freedom of choice when
                        deciding which database to use, as long as it is compatible to trino.
                    </p>
                </div>
                <div class="column">
                    <figure class="image">
                        <a href="../images/lakeFS_DBeaver_homescreen.png" class="glightbox">
                            <img src="../images/lakeFS_DBeaver_homescreen.png" alt="Any DB picture">
                        </a>
                    </figure>
                </div>
            </div>

            <!-- organization =
                    Cloud provider - lakefs instance - mehrere Repos - je mehrere branches  - Verzeichnisse
                    trino ways
            -->
            <div class="columns">
                <div class="column">
                    <p>
                        The <strong>organization</strong> can be seen out of two equivalent perspectives.
                        On the one hand there is the data structure, which is similar to the filesystem,
                        on the other hand there is the database structure.
                    </p>
                    <p>
                        From the <strong>data perspective</strong> we have five layers that can be seen:
                        <ul>
                            <li> Most basically the Cloud provider holds the data in a s3-compatible obejct storage</li>
                            <li> In that, we have the lakeFS instance, which is maintaining the saved data</li>
                            <li> Each lakeFS instance can have multiple repositories</li>
                            <li> In one repository there are multiple branches</li>
                            <li> And in those branches, the data is maintained within a file system</li>
                        </ul>
                    </p>
                </div>

                <div class="column">
                    <figure class="image">
                        <a href="../images/lakefs_filesystem.png" class="glightbox">
                            <img src="../images/lakefs_filesystem.png" alt="data perspective">
                        </a>
                    </figure>
                </div>
            </div>

            <!-- organization =
                    Cloud provider - lakefs instance - mehrere Repos - je mehrere branches  - Verzeichnisse
                    trino ways
            -->
            <div class="columns">
                <div class="column">
                    <p>
                        Another perspective is the <strong>database perspective</strong>:
                        <ul>
                            <li> Here most basically is the compute instance, which executes the trino sql statements</li>
                            <li> Those statements build trino's default database</li>
                            <li> Each database can have multiple schemas, where the team organizes their data,
                                like staging, dev, and data products</li>
                            <li> As usual each schema can have multiple tables</li>
                        </ul>
                        The simple organization option is to have a single lakeFS instance for all teams. In this instance each team can work in one repository, but all teams share one database with their own schema(s).
                        Inside a repository each team can decide on its own, how to structure their data and data products, within the given rules by
                        the federated governance, of course. To publish a final data product the table can just be saved under a common schema like <code>data_products</code>,
                        where other teams can look for data if needed.
                        While this is a good starting point, it is likely that more access management will be needed in a production environment.
                        Therefore this solution can be adjusted that each team works in its own lakeFS instance.
                        That way the isolation gets one level above and the data products can still be easily shared through configuration files in trino.
                        The access for other teams will then be regulated through trino profiles and read only lakeFS groups.
                        More precisely, if a team needs access to a table from another team, the database can be added to trino with a simple properties file, which optimally
                        would be provided by the domain team with predefined access rights.
                    </p>
                    <p>
                        Another charakteristic of this stack is that, because of the usage of multiple frameworks, there are also multiple instances of access management, that need to be configured.
                        First, most likely the cloud provider has an access management, then MinIO comes with an access management.
                        Lastly on top the storage lakeFS also has an access management.
                    </p>
                </div>

                <div class="column">
                    <figure class="image">
                        <a href="../images/lakefs_database_navigator.png" class="glightbox">
                            <img src="../images/lakefs_database_navigator.png" alt="database perspective">
                        </a>
                    </figure>
                </div>
            </div>

            <!-- data catalog = hive mit seinen vor und nachteilen -->
            <div class="columns">
                <div class="column">
                    <p>
                        Trino uses a <strong>built-in hive-catalog</strong> as a metadata storage. This is also accessible to discover and govern published data products.
                        Nonetheless we also recommend adding metadata tracking for the discovery of data products, like Openlineage.
                    </p>
                </div>
                <div class="column">
                    <figure class="image">
                        <a href="../images/unknown.png" class="glightbox">
                            <img src="../images/unknown.png" alt="screenshot hive catalog">
                        </a>
                    </figure>
                </div>
            </div>

            <!-- metadata tracking = entweder manuell über marquez oder mit dbt loggen lassen -->

            <div class="columns">
                <div class="column">
                    <p>
                        We recommend using a <strong>metadata tracking service</strong>, which can be used both to discover
                        available data products as well as debugging unusual data behaviour.
                        With metadata tracking the processing steps are tracked and it is possible
                        to discover the processing history of each dataset as well as metadata about it, which can be used and regulated in the federated governance.
                    </p>
                    <p>
                        For example, the data processing can be integrated with Marquez, a metadata tracking service, which implements the OpenLineage
                        standard for tracking metadata and schema information. Then for each dataset the schema information and processing step, which created it, is logged.
                        This allows visualizing the dependencies between datasets and processing steps (data lineage) in form of a graph.
                    </p>
                </div>
                <div class="column">
                    <figure class="image">
                        <a href="../images/unknown.png" class="glightbox">
                            <img src="../images/unknown.png" alt="screenshot marquez lineage graph">
                        </a>
                    </figure>
                </div>
            </div>

            <!-- dashboard = BYO was in JNBs funktioniert, über trino auch advanced bi tools -->

            <div class="columns">
                <div class="column">
                    <p>
                        As mentioned before a benefit of this tech stack is customizablility. This shows when visualizing the data, because
                        this stack does not come with a built-in dashboard for data analysis.
                        On the other hand it has many options to integrate packages or frameworks.
                        This way you can decide on our own if you like to integrate another framework, which trino has multiple options to,
                        or to prepare your own dashboards in jupyter notebooks with frameworks, like zeppelin and matplotlib.
                    </p>
                </div>
                <div class="column">
                    <figure class="image">
                        <a href="../images/unknown.png" class="glightbox">
                            <img src="../images/unknown.png" alt="screenshot of metadata catalog or combination query">
                        </a>
                    </figure>
                </div>
            </div>

            <!-- monitoring = trino overview evtl ergänzend MinIO Speicherplatz -->

            <div class="columns">
                <div class="column">
                    <p>
                        When it comes to <strong>monitoring</strong> this stack has a variety of monitoring tools.
                        First of all the performance and managing costs are most likely provided by the cloud provider. Additionally lakeFS as well as MinIO are offering a Prometheus integration for monitoring purposes.
                        In case of MinIO, the collected metrics from prometheus can then be visualized in the MinIO Homescreen.
                    </p>
                    <p>
                        Also Trino offers an interface that provides information about the queries, like ran, running, queued and blocked queries and performance
                        and workload measurements, like active workers, running drivers, reserved memory, rows and bytes/sec and worker parallelism.
                        Additionally there is an overview about every query, which can be sorted and searched through.
                    </p>


                </div>
                <div class="column">
                    <figure class="image">
                        <a href="../images/lakeFS_trino_overview.png" class="glightbox">
                            <img src="../images/lakeFS_trino_overview.png" alt="trino overview, maybe more">
                        </a>
                    </figure>
                </div>
            </div>

            <div>
                To recap this stack comes with the following characteristics:
                <ul>
                    <li>Highly flexible tech stack with endless variety of integration options</li>
                    <li>Similar to Snowflake when dbt is integrated</li>
                    <li>Fully open souce</li>
                    <li>Free of charge</li>
                    <li>High customizability through coding, with its benefits and challenges</li>
                </ul>
            </div>

            <h5 style="margin-top:1cm;">References</h5>
            <ul>
                <li><a href="https://docs.min.io">Official MinIO documentation</a></li>
                <li><a href="https://docs.lakefs.io">Official lakeFS documentation</a></li>
                <li><a href="https://trino.io/docs/current/">Official trino documentation</a></li>
                <li><a href="https://lakefs.io/category/integrations/">Various lakeFS blogposts to integration of lakeFS with MinIO and Trino</a></li>
            </ul>

        </div>
    </section>

</div>

<footer class="footer">
    <div class="content has-text-centered">
        <p>
            <a href="https://www.innoq.com">
                <img src="/images/supported-by-innoq--petrol-apricot.svg" alt="Supported by INNOQ" class="footer-logo" width="180" />
            </a>
        </p>
        <p>
            <a href="https://www.innoq.com/en/impressum/">Legal Notice</a>
            &nbsp
            <a href="https://www.innoq.com/en/datenschutz/">Privacy</a>
        </p>
    </div>
</footer>

<script src="/js/navigation.js"></script>

<script async defer src="https://scripts.simpleanalyticscdn.com/latest.js"></script>
<noscript><img src="https://queue.simpleanalyticscdn.com/noscript.gif" alt="" referrerpolicy="no-referrer-when-downgrade" /></noscript>

<link rel="stylesheet" href="/css/glightbox.css" />
<script src="/js/glightbox.js"></script>
<script type="text/javascript">
  const lightbox = GLightbox({});
</script>


<script src="/js/anchor.min.js"></script>
<script>anchors.add();</script>
</body>

</html>
