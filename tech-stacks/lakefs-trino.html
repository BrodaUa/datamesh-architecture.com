<!DOCTYPE html>
<html lang="en">
<head>
    <title>Data Mesh Architecture: Databricks</title>
    <meta charset="utf-8">
    <meta name="description" content="How to build a data mesh architecture with Databricks" />
    <meta name="keywords" content="data mesh, data mesh architecture, domain-driven data analytics, data analytics, domain-driven design, domain ownership, data as a product, data product, federated governance, self-serve data platform, data platform">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="twitter:card" content="summary_large_image" />
    <meta name="twitter:site" content="@innoq" />
    <meta name="twitter:title" content="Data Mesh Architecture: Databricks" />
    <meta name="twitter:description" content="How to build a data mesh architecture with Databricks" />
    <meta name="twitter:image" content="https://www.datamesh-architecture.com/images/databricks_card.png" />
    <meta name="twitter:image:alt" content="Data Mesh Architecture: Domains are in the center and teams do analytics on their own. They build and interconnect with data products. A data platform team and a enablement team help. Global policies are agreed through federated governance." />
    <meta property="og:url" content="https://datamesh-architecture.com" />
    <meta property="og:title" content="Data Mesh Architecture: Databricks" />
    <meta property="og:description" content="How to build a data mesh architecture with Databricks" />
    <meta property="og:image" content="https://www.datamesh-architecture.com/images/databricks_card.png" />

    <link rel="preload" as="font" type="font/woff2" href="https://www.innoq.com/assets/MarkPro-Book.woff2?cachebuster=2" crossorigin="">
    <link rel="preload" as="font" type="font/woff2" href="https://www.innoq.com/assets/MarkPro-Bold.woff2?cachebuster=2" crossorigin="">
    <link rel="preload" as="font" type="font/woff2" href="https://www.innoq.com/assets/MarkPro-Heavy.woff2?cachebuster=2" crossorigin="">
    <link rel="stylesheet" href="../css/style.css" />
    <link rel="stylesheet" href="../css/0.9.3_css_bulma.css" />
    <link rel="stylesheet" href="../css/font-awesome_6.0.0_css_all.css"/>
    <link rel="shortcut icon" type="image/x-icon" href="/favicon.ico">
</head>
<body>
<nav class="navbar is-dark" role="navigation" aria-label="dropdown navigation">
    <div class="container">
        <div class="navbar-brand">
                    <span class="navbar-burger" data-target="navbarMenuHeroA">
            <span></span>
            <span></span>
            <span></span>
          </span>
        </div>
        <div id="navbarMenuHeroA" class="navbar-menu">

            <div class="navbar-end">


                <a href="/#why" class="navbar-item">
                    Why
                </a>
                <a href="/#what-is-data-mesh" class="navbar-item">
                    What
                </a>
                <div class="navbar-item has-dropdown is-hoverable">
                    <a href="/#how-to-design-a-data-mesh" class="navbar-link is-arrowless">
                        How
                    </a>
                    <div class="navbar-dropdown" id="navbarMenuArchitectureDropdown">
                        <a href="/#how-to-design-a-data-mesh" class="navbar-item">Data Mesh Architecture</a>
                        <hr class="navbar-divider">
                        <a href="/#data-product" class="navbar-item">Data Product</a>
                        <a href="/#federated-governance" class="navbar-item">Federated Governance</a>
                        <a href="/#analytical-data" class="navbar-item">Analytical Data</a>
                        <a href="/#ingesting" class="navbar-item">Ingesting</a>
                        <a href="/#clean-data" class="navbar-item">Clean Data</a>
                        <a href="/#analytics" class="navbar-item">Analytics</a>
                        <a href="/#data-platform" class="navbar-item">Data Platform</a>
                        <a href="/#enabling-team" class="navbar-item">Enabling Team</a>
                    </div>
                </div>
                <a href="/#mesh" class="navbar-item">
                    Mesh
                </a>
                <div class="navbar-item has-dropdown is-hoverable">
                    <a href="/#tech-stacks" class="navbar-link is-arrowless">
                        Tech Stacks
                    </a>
                    <div class="navbar-dropdown" id="navbarMenuTechStackDropdown">
                        <a href="/tech-stacks/google-cloud-bigquery.html" class="navbar-item">
                            Google Cloud BigQuery
                        </a>
                        <a href="/tech-stacks/aws-s3-athena.html" class="navbar-item">
                            AWS S3 and AWS Athena
                        </a>
                        <a href="/tech-stacks/azure-synapse-analytics.html" class="navbar-item">
                            Azure Synapse Analytics
                        </a>
                        <a href="/tech-stacks/dbt-snowflake.html" class="navbar-item">
                            dbt and Snowflake
                        </a>
                        <a href="/tech-stacks/databricks.html" class="navbar-item">
                            Databricks
                        </a>
                    </div>
                </div>
                <div class="navbar-item has-dropdown is-hoverable">
                    <a href="/#domain-teams-journey" class="navbar-link is-arrowless">
                        Start the Journey
                    </a>
                    <div class="navbar-dropdown is-right" id="navbarMenuTransformationDropdown">
                        <a href="/#domain-teams-journey" class="navbar-item">
                            Domain Team’s Journey
                        </a>
                        <a href="/#data-teams-journey" class="navbar-item">
                            Data Team’s Journey
                        </a>
                        <a href="/real-world-learnings.html" class="navbar-item">
                            Real World Learnings
                        </a>
                    </div>
                </div>
            </div>
        </div>
    </div>
</nav>

<div class="container">

    <section class="section">

        <nav class="breadcrumb" aria-label="breadcrumbs">
            <ul>
                <li><a href="/">Data Mesh Architecture</a></li>
                <li><a href="/#tech-stacks">Tech Stacks</a></li>
                <li class="is-active"><a href="#" aria-current="page">Databricks</a></li>
            </ul>
        </nav>

        <h1 class="title">LakeFS and Trino</h1>

        <div class="notification is-info is-light">
            Data mesh is primarily an organizational approach, and that's why you can't buy a data mesh from a vendor. Technology, however, is important still as it acts as an enabler for data mesh, and only useful and easy to use solutions will lead to domain teams' acceptance. The available offerings of cloud providers already provide a sufficient set of good self-serve data services to let you form a data platform for your data mesh. We want to show which services can be used to get started.
        </div>

        <div class="content">
            <p>
                The following LakeFS with Trino tech stack is an alternative open source solution for a data mesh platform in a data mesh, which can be combined with MinIO as a storage.
                This stack, as it is, was used by INNOQ in a former PoC, thus it does have some optional expansions that were not actually used, but we will mention them for the sake of completeness.
            </p>
            <p>
                <a href="https://lakefs.io">LakeFS</a> is a git-like data versioning tool that recently has emerged high popularity.
                LakeFS is a layer on top of the storage solution, which is, per default, providing a filesystem view on the data.
                With lakeFS, <strong>common git operations</strong> like branching and merging and are added to the functionality of the storage solution.
                This branching functionality is providing <strong>isolation without copying data</strong> and can be used to create lasting snapshots of the
                data or in the case of a data mesh, a data product.
            </p>
            <p>
                <a href="https://trino.io">Trino</a> is a fast and distributed query engine.
                Thereby trino works with a hive metastore <strong>to support SQL-based data processing</strong>.
                With trino we have the ability to mix databases in single SQL-statements due to the support of using multiple databases.
            </p>
            <p>
                <a href="https://trino.io">MinIO</a> is the complementary object storage.
                MinIO is a good choice because of a good interoperability with lakeFS combined with the s3-compatibility, which
                allows using most of the large cloud providers like AWS, Azure and Google.
            </p>
            <p>

            </p>

            <strong>WIP:</strong>
            <img src="../images/lakefs_trino.webp" alt="Data Mesh Architecture with LakeFS and Trino" style="width: 100%">


            <!--
            <ul>
                 <li>LakeFS with Trino is an alternative open source tech stack</li>
                <li>The stack consists of three main frameworks:</li>

                <ul>
                    <li>MinIO and an s3-compatible object storage</li>
                    <li>LakeFS as a layer on top of the object storage</li>
                    <li>Trino is the distibuted query engine that can combine multiple datawarehouses</li>
                </ul>

                <li>Fully open source and highly customizable through expansions like</li>
                <ul>
                    <li>dbt which gives various benefits as described in the snowflake stack</li>
                    <li>openlineage which is especially useful if the data lineage is necessary</li>
                </ul>

                <li>can be initialized in a local setup or use cloud resources (Object Storage, Compute, Access) by a cloud provider</li>
            </ul>
            -->

            <!-- intro setup with jnbs and dbt -->
            <div class="columns">
                <div class="column">
                    <p>
                        This stack is combines the tools lakeFS and trino. This combination has the main benefit of providing
                        data versioning to this data mesh stack. Furthermore, this stack comes with a very high customizability,
                        which, most likely, is both beneficial as well as a challenging factor.
                    </p>
                    <p>
                        Overall this stack is very leightweight and wrapped into a docker environment.
                        This also means that at first it has to be configured in the docker configuration file, which is an initial configuration effort.
                        Once this setup is up and running, the transformation and cleaning of data can begin.
                    </p>
                </div>
                <div class="column">
                    <figure class="image">
                        <a href="../images/lakeFS_dockercontainers.png" class="glightbox">
                            <img src="../images/lakeFS_dockercontainers.png" alt="Docker container overview">
                        </a>
                    </figure>
                </div>
            </div>

            <!-- trino allows sql based access on the data based storage -->
            <div class="columns">
                <div class="column">
                    <p>
                        The storage stack gets combined with <strong>trino, a fast and distributed query engine,</strong> that provides
                        sql-based access to the data-based filesystem.
                        This is also the tool that uses the local or cloud computing resources. This variation between a local and cloud setup thereby
                        supports a simple initial setup as well as beeing a viable production solution.
                    </p>
                    <p>
                        Trino comes with the ability to combine tables from multiple databases in one sql statement, which is useful in the data mesh context.
                        <br/>
                        One the one hand, this is useful during the setup process, when the database gets built or transfered, because, if the old database solution is
                        compatible with trino, it can be simply transfered or fully kept if necessary.
                        </br>
                        On the other hand, it is useful when publishing data products, because it gives the domain teams a freedom of choice when
                        deciding which database to use, as long as it is compatible with the trino connectors.
                    </p>
                </div>
                <div class="column">
                    <figure class="image">
                        <a href="../images/lakeFS_DBeaver_homescreen.png" class="glightbox">
                            <img src="../images/lakeFS_DBeaver_homescreen.png" alt="Any DB picture">
                        </a>
                    </figure>
                </div>
            </div>

            <!-- setup with jnbs and dbtools -->
            <div class="columns">
                <div class="column">
                    <p>
                        The trino engine will be used for transformation and data cleaning, which will be done in <strong>jupyter notebooks</strong>.
                        </br>
                        The combination of jupyter notebooks with trino results a <strong>powerful and flexible IDE for exploratory analysis</strong> with python toolings.
                        Additionally using a database tool can help with an extra exploratory view on the data, but we recommend jupyter notebooks as
                        they can be easily used for exploratory as well as production purposes.
                    </p>
                    <p>
                        To <strong>design a data product</strong>, we suggest a workflow that starts with an exploratory phase in jupyter notebooks or database tooling.
                        When this exploratory phase leads to an accepted data product, they can be transfered into persistent jupyter notebooks or dbt models.
                        <br/>
                        As an expansion, dbt can be used as it allows an easy setup of a persistent production workflow.
                        Although the data build tool, short dbt, is an optional tool in this tech stack, with dbt it is easy to automate the ELT-processes,
                        that build the data products.
                        This comes in handy when setting up and designing data products as it makes design and maintenance of data products even easier.
                    </p>

                </div>
                <div class="column">
                    <figure class="image">
                        <a href="../images/lakeFS_trino_query.png" class="glightbox">
                            <img src="../images/lakeFS_trino_query.png" alt="Jupyter notebook demo query">
                        </a>
                    </figure>
                </div>
            </div>

            <!-- data product = trino Tabelle die mit Daten versorgt wird -->
            <div class="columns">
                <div class="column">
                    <p>
                        In a data mesh, <strong>every domain team manages their own jupyter notebooks</strong> and resulting tables in their database.
                        Also, when publishing a data product, usually a table or view, it gets marked as a final <em>data_product</em>. Thereby its beneficial
                        to put them into a dedicated schema like <code>data_products</code>, which can be saved in a separate folder in lakeFS.
                        <br/>
                        To publish the data product that or any sub folder with the raw data can be published via lakeFS, but we recommend to use trino.
                        With trino, <strong>using data products from other domains</strong> can be easily referred through the ability to combine tables
                        from multiple sources in the same sql statement.
                        <br/>
                        With that, you can just configure the fitting trino connector and just use the data product in your next sql statement.
                    </p>
                </div>
                <div class="column">
                    <figure class="image">
                        <a href="../images/lakefs_jupyter_list.png" class="glightbox">
                            <img src="../images/lakefs_jupyter_list.png" alt="juypter tree">
                        </a>
                    </figure>
                </div>
            </div>

            <!-- complementary
            <div class="columns">
                <div class="column">
                    <p>
                        <strong>Creation of data products:</strong>
                    <ul>
                        <li> in notebooks: highly customizable through code, including import, transform, validation, and export;</li>
                            <ul>
                                <li>lots of freedom (sql, python, ...);</li>
                                <li>custom step to publish data product description may be helpful (dynamic infos)</li>
                            </ul>
                        <li> with dbt: sql statements in dbt models </li>
                        <ul>
                            <li> sql only but benefits of dbt included</li>
                            <li> custom and predefined tests</li>
                            <li> dbt models as metadata information</li>
                        </ul>
                    </ul>
                    </p>
                    <p>
                        <strong>The sharing of data products:</strong>
                        <ul>
                            <li> easy: done within trino via just accessing database tables of another teams database</li>
                            <li> intermediate: with access control in lakeFS and sharing of snapshot branches whereby metadata needs to be published accordingly</li>
                        </ul>
                    </p>
                    <p style="margin-top:1cm;">
                        <strong>Downside:</strong>
                    <li> Mental load is high-medium to very high through the infinite amount of customizability</li>
                    </p>
                </div>

                <div class="column">
                    <figure class="image">
                        <a href="../images/lakeFS_trino_query.png" class="glightbox">
                            <img src="../images/lakeFS_trino_query.png" alt="notebook trino statement">
                        </a>
                    </figure>
                    <figure class="image">
                        <a href="../images/lakeFS_dbt_query.png" class="glightbox">
                            <img src="../images/lakeFS_dbt_query.png" alt="dbt model statement">
                        </a>
                    </figure>
                </div>
            </div>
            -->

            <!-- stores = lakefs in branches, darunter MinIO was auf s3 liegt -->
            <div class="columns">
                <div class="column">
                    <p>
                        In our PoC, we used a local MinIO setup that uses the local storage, but
                        this can be replaced by <strong>any s3-compatible object storage</strong> for scaling.
                        Besides that connection, MinIO provides the <strong>MinIO Azure Gateway</strong>, which allows to connect to non-s3-compatible storage like the Azure Data Lake Gen 1.
                    </p>
                    <p>
                        The object storage gets covered by lakeFS, the version control system.
                        With lakeFS the underlying object storage is saving various non-assignable data files that are mapped by
                        lakeFS to construct a filesystem for the user.
                    </p>
                </div>
                <div class="column">
                    <figure class="image">
                        <a href="../images/lakeFS_minio_homescreen.png" class="glightbox">
                            <img src="../images/lakeFS_minio_homescreen.png" alt="MinIO Demoscreen">
                        </a>
                    </figure>
                </div>
            </div>

            <!-- data versioning =  infos über lakefs -->
            <div class="columns">
                <div class="column">
                    <p>
                        The <strong>speciality</strong> of this stack is the <strong>data versioning</strong> that LakeFS provides.
                    </p>
                    <p>
                        <strong>LakeFS is a data version control system</strong>, which provides git-like operations for the data lake.
                        With the branching functionality it is easy to create isolated snapshots of specific current data.
                        <br/>
                        In a data mesh, this branching can be used either as a data product, which gets published,
                        or as an exploration branch, where developers can try out future data products, with the mindset, that the production data can't be corrupted.
                    </p>
                    <p>
                        A major benefit thereby is that while branching <strong>no data gets copied</strong>.
                        <br/>
                        Furthermore it comes with the ability to <strong>time travel</strong>.
                        If anything goes wrong, the affected branch can easily be reverted to an older state.
                        Another advantage is, that lakeFS will tackle the eventual consistency of streams by confirming stored data due to committing.
                    </p>
                </div>
                <div class="column">
                    <figure class="image">
                        <a href="../images/lakefs_branches_list.png" class="glightbox">
                            <img src="../images/lakefs_branches_list.png" alt="Lakefs branches picture">
                        </a>
                    </figure>
                </div>
            </div>

            <!-- organization =
                    Cloud provider - lakefs instance - mehrere Repos - je mehrere branches  - Verzeichnisse
                    trino ways
            -->
            <!--
            <div class="columns">
                <div class="column">
                    <p>
                        The <strong>organization</strong> can be seen out of two equivalent perspectives.
                        On the one hand there is the data structure, which is similar to the filesystem,
                        on the other hand there is the database structure.
                    </p>
                    <p>
                        From the <strong>data perspective</strong> we have five layers that can be seen:
                        <ul>
                            <li> Most basically the Cloud provider holds the data in a s3-compatible obejct storage</li>
                            <li> In that, we have the lakeFS instance, which is maintaining the saved data</li>
                            <li> Each lakeFS instance can have multiple repositories</li>
                            <li> In one repository there are multiple branches</li>
                            <li> And in those branches, the data is maintained within a file system</li>
                        </ul>
                    </p>
                        Another perspective is the <strong>database perspective</strong>:
                        <ul>
                            <li> Here most basically is the compute instance, which executes the trino sql statements</li>
                            <li> Those statements build trino's default database</li>
                            <li> Each database can have multiple schemas, where the team organizes their data,
                                like staging, dev, and data products</li>
                            <li> As usual each schema can have multiple tables</li>
                        </ul>

                </div>

                <div class="column">
                    <figure class="image">
                        <a href="../images/lakefs_filesystem.png" class="glightbox">
                            <img src="../images/lakefs_filesystem.png" alt="data perspective">
                        </a>
                    </figure>
                </div>
            </div>
            -->

            <!-- organization =
                    Cloud provider - lakefs instance - mehrere Repos - je mehrere branches  - Verzeichnisse
                    trino ways
            -->
            <div class="columns">
                <div class="column">
                    <p>
                        The <strong>organization</strong> of this stack can also have a simple version or an advanced one for production.
                        <br/>
                        The </strong>simple organization</strong> of this stack is to have a single lakeFS instance for all teams.
                        In this instance each team can work in its own repository, but all teams share one database.
                        The teams can set up their own schemas, but these can not be access controlled to the other teams.
                        <br/>
                        Inside this repository each team can decide on its own, how to structure their data and data products, within the given rules by
                        the federated governance. To publish a final data product the table can just be saved under the common schema like <code>data_products</code>,
                        where the other teams can look for data if needed.
                        <br/><br/>
                        While this is a good starting point, it is likely that more access management will be needed in a production environment.
                        Therefore this solution can be adjusted to an <strong>advanced version</strong>, where each team works in its own lakeFS instance.
                        That way the isolation gets rised one level above and the data products can still be easily shared through trino.
                        The access for other teams will then be regulated through trino profiles with read only access.
                        More precisely, if a team needs access to a table from another team, the database can be added to trino with a simple properties file,
                        which optimally would be provided by the owning domain team with predefined access rights.
                    </p>
                    <!--
                    <p>
                        Another charakteristic of this stack is that, because of the usage of multiple frameworks, there are also multiple instances of access management, that can be configured.
                        We recommend using the access management of the cloud provider, which can be set up by the enabling or data platform team.
                        But additionally MinIO and lakeFS also come with a simple access management.
                    </p>
                    -->
                </div>

                <div class="column">
                    <figure class="image">
                        <a href="../images/lakefs_database_navigator.png" class="glightbox">
                            <img src="../images/lakefs_database_navigator.png" alt="database perspective">
                        </a>
                    </figure>
                </div>
            </div>

            <!-- data catalog = hive mit seinen vor und nachteilen -->
            <div class="columns">
                <div class="column">
                    <p>
                        Trino uses a <strong>built-in hive-catalog</strong> as a metadata storage.
                        <br/>
                        This catalog is accessible on a database and could be used to discover and govern published data products.
                        Nonetheless, we recommend using metadata tracking for an easier and visually more pleasing discovery of data products.
                    </p>
                </div>
                <div class="column">
                    <figure class="image">
                        <a href="../images/lakefs_dbeaver_metadatadb.png" class="glightbox">
                            <img src="../images/lakefs_dbeaver_metadatadb.png" alt="screenshot hive catalog">
                        </a>
                    </figure>
                </div>
            </div>

            <!-- metadata tracking = entweder manuell über marquez oder mit dbt loggen lassen -->

            <div class="columns">
                <div class="column">
                    <p>
                        As mentioned, we recommend a <strong>metadata tracking service</strong>, which can be used to discover
                        available data products as well as using the data lineage to debug unusual data behaviour.
                        With metadata tracking, the processing steps of a data product can be tracked. This allows to discover the processing history
                        as well as metadata of each dataset in a visually way. This also can be used and regulated in the federated governance.
                    </p>
                    <p>
                        As an example, the data processing can be integrated with Marquez, a metadata tracking service that implements the Openlineage standard.
                        For each dataset the schema information and processing step, which created it, can be logged, which allows visualizing the dependencies
                        between datasets and processing steps (data lineage) in form of a visual graph.
                    </p>
                </div>
                <div class="column">
                    <figure class="image">
                        <a href="../images/lakefs_datalineage_graph.png" class="glightbox">
                            <img src="../images/lakefs_datalineage_graph.png" alt="screenshot marquez lineage graph">
                        </a>
                    </figure>
                </div>
            </div>

            <!-- dashboard = BYO was in JNBs funktioniert, über trino auch advanced bi tools -->

            <div class="columns">
                <div class="column">
                    <p>
                        Another benefit of this tech stack is the <strong>high customizablility</strong>. This benefit shows when it comes to visualizing the data, because
                        this stack does not come with a built-in dashboard for data analysis.
                        It has <strong>many options to integrate packages or frameworks</strong>.
                        We consider this a benefit, because you can decide on our own to prepare your own dashboards in jupyter notebooks, with frameworks like
                        Apache Zeppelin and the matplotlib, or if you like to integrate another framework via the JDBC endpoint that, like Apache Superset.
                        Additionally trino gives various options, like PowerBI, Tableau, Looker, Sisense, by allowing JDBC access.
                    </p>
                </div>
                <div class="column">
                    <figure class="image">
                        <a href="../images/lakeFS_jupyter_plot.png" class="glightbox">
                            <img src="../images/lakeFS_jupyter_plot.png" alt="screenshot of a visual demo graph in jupyter notebook">
                        </a>
                    </figure>
                </div>
            </div>

            <!-- monitoring = trino overview evtl ergänzend MinIO Speicherplatz -->

            <div class="columns">
                <div class="column">
                    <p>
                        When it comes to <strong>monitoring</strong> this stack has a variety of monitoring tools.
                        <br/>
                        First of all the performance and managing costs are most likely provided by the individual cloud provider.
                        Additionally lakeFS as well as MinIO are offering a Prometheus integration for monitoring purposes.
                        In case of MinIO, the collected metrics from Prometheus can even be visualized back in the MinIO Homescreen.
                    </p>
                    <p>
                        Furthermore trino offers an interface that provides information about queries. In this cluster overview you can track which queries were ran,
                        are still running, queued or blocked together with performance and workload measurements, like active workers, running drivers, reserved memory,
                        rows and bytes/sec and worker parallelism.
                        Additionally there is an list about every query, which can be inspected, sorted and searched through.
                    </p>


                </div>
                <div class="column">
                    <figure class="image">
                        <a href="../images/lakeFS_trino_overview.png" class="glightbox">
                            <img src="../images/lakeFS_trino_overview.png" alt="trino overview, maybe more">
                        </a>
                    </figure>
                </div>
            </div>

            <div>
                This stack is a powerful and highly flexible tech stack with an endless variety of integration possibilities.
                Still, these possibilities come with the downside of complexity of the system, which should, in the principles of a data mesh, be as simple to understand as possible.
                The implementation of data products comes with a high mental load, in comparison to the other tech stacks.
                <br/>
                Nonetheless, this tech stack can be seen as a good allrounding building block to stick with programms and frameworks, which are already in use in the current stack.
                Additionally, this stack is completely open source and, at least in the local environment, fully free of charge, which makes it always a good option to try out, when thinking about setting up a data mesh.
                <br/>
                All in all the high customizability and flexibility of this stack comes with its benefits and challenges, which are mostly caused by the high amount of possibilities this stack can offer.

            </div>

            <h5 style="margin-top:1cm;">References</h5>
            <ul>
                <li><a href="https://docs.min.io">Official MinIO documentation</a></li>
                <li><a href="https://docs.lakefs.io">Official lakeFS documentation</a></li>
                <li><a href="https://trino.io/docs/current/">Official trino documentation</a></li>
                <li><a href="https://lakefs.io/category/integrations/">Various lakeFS blogposts to integration of lakeFS with MinIO and Trino</a></li>
                <br/>
                <li><a href="https://prometheus.io">Prometheus website</a></li>
                <li><a href="https://openlineage.io">Openlineage website</a></li>
                <li><a href="https://marquezproject.github.io/marquez/quickstart.html">Marquez github page</a></li>
                <br/>
                <li><a href="https://jupyter.org">Jupyter website</a></li>
                <li><a href="https://dbeaver.io">DBeaver website</a></li>
                <br/>
                <li><a href="https://zeppelin.apache.org">Official Apache Zeppelin documentation</a></li>
                <li><a href="https://matplotlib.org/stable/">Official matplotlib documentation</a></li>
                <li><a href="https://superset.apache.org">Official Apache Superset documentation</a></li>
                <li><a href="https://pandio.com">Official Pandio website</a></li>



            </ul>

        </div>
    </section>

</div>

<footer class="footer">
    <div class="content has-text-centered">
        <p>
            <a href="https://www.innoq.com">
                <img src="/images/supported-by-innoq--petrol-apricot.svg" alt="Supported by INNOQ" class="footer-logo" width="180" />
            </a>
        </p>
        <p>
            <a href="https://www.innoq.com/en/impressum/">Legal Notice</a>
            &nbsp
            <a href="https://www.innoq.com/en/datenschutz/">Privacy</a>
        </p>
    </div>
</footer>

<script src="/js/navigation.js"></script>

<script async defer src="https://scripts.simpleanalyticscdn.com/latest.js"></script>
<noscript><img src="https://queue.simpleanalyticscdn.com/noscript.gif" alt="" referrerpolicy="no-referrer-when-downgrade" /></noscript>

<link rel="stylesheet" href="/css/glightbox.css" />
<script src="/js/glightbox.js"></script>
<script type="text/javascript">
  const lightbox = GLightbox({});
</script>


<script src="/js/anchor.min.js"></script>
<script>anchors.add();</script>
</body>

</html>
